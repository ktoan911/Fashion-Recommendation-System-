# ğŸš€ Fashion VLP Training Optimization Guide

HÆ°á»›ng dáº«n tá»‘i Æ°u hÃ³a training Ä‘á»ƒ tÄƒng tá»‘c **3-5x** so vá»›i version gá»‘c.

## ğŸ“‹ CÃ¡c Tá»‘i Æ¯u HÃ³a ÄÃ£ Implement

### 1. ğŸ”„ DataLoader Optimization
- **num_workers=4**: Song song hÃ³a data loading
- **pin_memory=True**: Tá»‘i Æ°u GPU memory transfer  
- **persistent_workers=True**: Giá»¯ workers alive
- **prefetch_factor=2**: Prefetch data batches

### 2. ğŸ¯ Mixed Precision Training (AMP)
- Giáº£m 50% GPU memory usage
- TÄƒng tá»‘c 1.5-2x training speed
- Sá»­ dá»¥ng flag `--mixed-precision`

### 3. ğŸ“Š Gradient Accumulation
- Train vá»›i effective batch size lá»›n hÆ¡n
- Cáº£i thiá»‡n gradient stability
- Sá»­ dá»¥ng `--gradient-accumulation N`

### 4. ğŸ§  Preprocessing Caching
- Cache káº¿t quáº£ clothes detection & landmark detection
- TrÃ¡nh tÃ­nh toÃ¡n láº¡i má»—i epoch
- TÄƒng tá»‘c data loading 3-5x

### 5. ğŸ“ˆ Learning Rate Scheduling
- OneCycleLR cho convergence nhanh hÆ¡n
- Adaptive learning rate decay

### 6. âš¡ Model Compilation (PyTorch 2.0+)
- `torch.compile()` cho extra speedup
- Automatic optimization

### 7. ğŸ›ï¸ Fast Validation
- Chá»‰ evaluate subset validation data
- Giáº£m thá»i gian validation 5-10x

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### Training CÆ¡ Báº£n (Optimized)
```bash
python train_optimized.py \
    --path-file /path/to/annotations \
    --path-folder /path/to/images \
    --batch 64 \
    --epochs 100
```

### Training vá»›i Mixed Precision + Gradient Accumulation
```bash
python train_optimized.py \
    --path-file /path/to/annotations \
    --path-folder /path/to/images \
    --batch 32 \
    --gradient-accumulation 4 \
    --mixed-precision \
    --fast-validation \
    --epochs 100
```

### Benchmark Performance
```bash
python benchmark_training.py \
    --path-file /path/to/annotations \
    --path-folder /path/to/images \
    --epochs 5 \
    --batch 32
```

## ğŸ“Š Expected Performance Improvements

| Configuration | Memory Usage | Training Speed | Convergence |
|---------------|--------------|---------------|-------------|
| Original | 100% | 1x | Baseline |
| + DataLoader Opt | 100% | 1.5x | Same |
| + Mixed Precision | 50% | 2.5x | Same |
| + Gradient Accum | 40% | 3x | Better |
| + Caching | 50% | 4-5x | Same |
| **All Combined** | **40-50%** | **4-6x** | **Better** |

## ğŸ”§ Tuning Parameters

### Batch Size & Gradient Accumulation cho GPU 15GB
```bash
# ğŸš€ Maximum Speed (15GB GPU)
--batch 32 --gradient-accumulation 4  # effective batch = 128

# ğŸ’¾ Memory Efficient (15GB GPU)  
--batch 16 --gradient-accumulation 8  # effective batch = 128

# ğŸ¯ Best Accuracy (15GB GPU)
--batch 64 --gradient-accumulation 2  # effective batch = 128

# âš ï¸ Náº¿u GPU memory nhá» hÆ¡n (8GB)
--batch 8 --gradient-accumulation 16  # effective batch = 128
```

### Cache Directory
```bash
# Specify custom cache location
--cache-dir /fast/ssd/cache
```

### Validation Frequency
```bash
# Validate má»—i 5 epochs thay vÃ¬ 10
--checkpoint-interval 5
```

## ğŸ› Troubleshooting

### 1. CUDA Out of Memory
```bash
# Giáº£m batch size, tÄƒng gradient accumulation
--batch 16 --gradient-accumulation 4
```

### 2. Cache Creation Slow
- Cache chá»‰ táº¡o láº§n Ä‘áº§u tiÃªn
- Láº§n cháº¡y tiáº¿p theo sáº½ nhanh hÆ¡n ráº¥t nhiá»u
- CÃ³ thá»ƒ cháº¡y song song cho train/val cache

### 3. Model Compilation Failed
- BÃ¬nh thÆ°á»ng trÃªn PyTorch < 2.0
- Training váº«n hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng

### 4. DataLoader Workers Error
```bash
# Giáº£m num_workers náº¿u gáº·p lá»—i
# Edit trong code: num_workers=2 hoáº·c num_workers=0
```

## ğŸ¯ Best Practices

### 1. ğŸš€ For Maximum Speed (15GB GPU)
```bash
python train_optimized.py \
    --mixed-precision \
    --gradient-accumulation 4 \
    --fast-validation \
    --batch 32
```

### 2. ğŸ’¾ For Memory Efficient (15GB GPU)
```bash
python train_optimized.py \
    --mixed-precision \
    --gradient-accumulation 8 \
    --batch 16 \
    --fast-validation
```

### 3. ğŸ¯ For Best Accuracy (15GB GPU)
```bash
python train_optimized.py \
    --mixed-precision \
    --gradient-accumulation 2 \
    --batch 64 \
    --checkpoint-interval 5
```

### 4. ğŸ“Š GPU Memory Usage Estimates (15GB)

| Batch Size | Mixed Precision | Memory Usage | Khuyáº¿n nghá»‹ |
|------------|-----------------|--------------|-------------|
| 8 | âœ… | ~3GB | Conservative |
| 16 | âœ… | ~6GB | Safe |
| 32 | âœ… | ~12GB | **Optimal** |
| 64 | âœ… | ~14GB | Near limit |
| 128 | âœ… | ~18GB | âŒ OOM |

## ğŸ“ File Structure

```
Fashion-Recommendation-System-/
â”œâ”€â”€ train.py                    # Original training script
â”œâ”€â”€ train_optimized.py          # â­ Optimized training script
â”œâ”€â”€ benchmark_training.py       # Performance comparison
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ fashioniq_dataset.py    # Original dataset
â”‚   â””â”€â”€ cached_fashioniq_dataset.py  # â­ Cached dataset
â”œâ”€â”€ cache/                      # ğŸ’¾ Preprocessing cache
â”‚   â”œâ”€â”€ train/
â”‚   â””â”€â”€ val/
â””â”€â”€ checkpoints/                # ğŸ’¾ Model checkpoints
```

## ğŸ“ˆ Performance Monitoring

Training script sáº½ hiá»ƒn thá»‹:
- Real-time loss vÃ  learning rate
- Epoch time vÃ  validation time  
- Memory usage (náº¿u available)
- Cache loading progress

## ğŸ”„ Migration tá»« Original

1. **Backup** checkpoints hiá»‡n táº¡i
2. Cháº¡y `train_optimized.py` vá»›i cÃ¹ng arguments
3. Cache sáº½ Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng
4. So sÃ¡nh performance vá»›i `benchmark_training.py`

## ğŸ’¡ Tips

- **Cache**: Sá»­ dá»¥ng SSD cho cache directory Ä‘á»ƒ tÄƒng tá»‘c I/O
- **GPU**: KÃ­ch hoáº¡t Mixed Precision trÃªn GPU Volta+ (RTX 20xx+)
- **CPU**: Äiá»u chá»‰nh `num_workers` theo sá»‘ cores
- **Memory**: Monitor GPU memory vÃ  Ä‘iá»u chá»‰nh batch size
- **Validation**: Sá»­ dá»¥ng `--fast-validation` trong development

## ğŸ† Expected Results

Vá»›i cÃ¡c optimizations nÃ y, báº¡n cÃ³ thá»ƒ expect:
- **3-5x** faster training speed
- **40-50%** less GPU memory usage  
- **Better convergence** vá»›i learning rate scheduling
- **Significant time savings** cho iterative development

ChÃºc báº¡n training thÃ nh cÃ´ng! ğŸš€